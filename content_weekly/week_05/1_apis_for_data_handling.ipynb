{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some lessons on working with real world data: APIs and data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to work towards being able\n",
    "to grab data on the fly from the USGS website. And now we've defined\n",
    "the site id for the Verde River, as well as some start and end dates\n",
    "to get the data for. With those defined clearly it makes it much\n",
    "easier for someone else to understand what you are trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'site_no': '09506000',\n",
    "    'begin_date': '2021-09-25',\n",
    "    'end_date': '2022-09-25'\n",
    "}\n",
    "query = urllib.parse.urlencode(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use f-strings to insert these values into the query URL\n",
    "which will point to the same website that we saw in the lecture portion\n",
    "You can verify this by copying the URL into your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verde_url = (\n",
    "    f'https://nwis.waterdata.usgs.gov/usa/nwis/dv/?'\n",
    "    f'cb_00060=on&format=rdb&referred_module=sw&{query}'\n",
    "\n",
    ")\n",
    "print(verde_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we need to download the data and get it into pandas.\n",
    "To download the data we'll use the `urllib` module which is \n",
    "built into the python \"standard library\" of stuff you get for\n",
    "free when you install python. We use the `urllib.request.urlopen`\n",
    "function which simply opens a connection to the url, just like \n",
    "going to the url in your web browser. Then, we can put the `response`\n",
    "into `pd.read_table`. There are a lot of other parameters going \n",
    "into this function now, and this is very common for when you scrape\n",
    "data directly from the internet because formats vary.\n",
    "\n",
    "Anyways, let's walk through a few of them:\n",
    " - comment='#': Lines beginning with a '#' are comments that pandas should ignore\n",
    " - skipfooter=1: Skip the last line, not including this leads to no data ¯\\_( ツ )_/¯\n",
    " - sep='\\s+': The separator is whitespace (one or more spaces) - this is a regular expression (or regex)\n",
    " - names: The names of the columns. I set these because the USGS ones are trash\n",
    " - index_col=2: Set the 3rd column as the index (that is, \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(verde_url)\n",
    "df = pd.read_table(\n",
    "    response,\n",
    "    comment='#',\n",
    "    skipfooter=1,\n",
    "    sep='\\t',\n",
    "    names=['agency', 'site', 'date', 'streamflow', 'quality_flag'],\n",
    "    index_col=2,\n",
    "    engine='python'\n",
    ").iloc[2:]\n",
    "df.index = pd.DatetimeIndex(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the streamflow data to floats and\n",
    "the index to datetimes. When processing raw data\n",
    "it's common to have to do some extra postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['streamflow'] = df['streamflow'].astype(np.float64)\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila - we have a nice dataframe with streamflow data in it\n",
    "You can do all of the standard pandas stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['streamflow'].plot()\n",
    "plt.semilogy()\n",
    "plt.ylabel('Streamflow [cfs]')\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you might be thinking... why only one year of data?!\n",
    "And that's a great question. Before getting too far, let's\n",
    "just turn our little data processing things into some helper\n",
    "functions to save space, and make this easier for you to port\n",
    "to your homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_usgs_url(site_no, begin_date, end_date):\n",
    "    return (\n",
    "        f'https://nwis.waterdata.usgs.gov/usa/nwis/dv/?'\n",
    "        f'cb_00060=on&format=rdb&referred_module=sw&'\n",
    "        f'site_no={site_no}&'\n",
    "        f'begin_date={begin_date}&'\n",
    "        f'end_date={end_date}'\n",
    "    )\n",
    "\n",
    "def open_usgs_data(site, begin_date, end_date):\n",
    "    url = create_usgs_url((site), begin_date, end_date)\n",
    "    response = urllib.request.urlopen(url)\n",
    "    df = pd.read_table(\n",
    "        response,\n",
    "        comment='#',\n",
    "        skipfooter=1,\n",
    "        sep='\\t',\n",
    "        names=['agency', 'site', 'date', 'streamflow', 'quality_flag'],\n",
    "        index_col=2,\n",
    "        engine='python'\n",
    "    ).iloc[2:]\n",
    "\n",
    "    # Now convert the streamflow data to floats and\n",
    "    # the index to datetimes. When processing raw data\n",
    "    # it's common to have to do some extra postprocessing\n",
    "    df['streamflow'] = df['streamflow'].astype(np.float64)\n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = '09506000'\n",
    "begin_date = '2000-09-25'\n",
    "end_date = '2022-09-25'\n",
    "\n",
    "df = open_usgs_data(site, begin_date, end_date)\n",
    "df['streamflow'].plot()\n",
    "plt.semilogy()\n",
    "plt.ylabel('Streamflow [cfs]')\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, 20 years of daily data starts to blend together.\n",
    "Just a quick aside on another nice feature of pandas, the resample\n",
    "Let's just resample the data down to monthly means like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b542050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['streamflow'].resample('M').mean().plot()\n",
    "plt.semilogy()\n",
    "plt.ylabel('Streamflow [cfs]')\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some DayMet data. \n",
    "Before that, let me explain...\n",
    "\n",
    "DayMet actually has a really nice interactive\n",
    "API explorer that you can use to prototype:\n",
    "- https://daymet.ornl.gov/single-pixel/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://daymet.ornl.gov/single-pixel/api/data?lat=34.9455&lon=-113.2549\"  \\\n",
    "       \"&vars=prcp&start=1984-08-14&end=2014-10-18&format=json\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = '2000-09-20'\n",
    "end_date = '2020-09-25'\n",
    "args = {\n",
    "    'lat':  34.4483605,\n",
    "    'lon': -111.7898705,\n",
    "    'start': begin_date,\n",
    "    'end': end_date,\n",
    "    'vars': ['prcp', 'Tmax', 'Tmin'],\n",
    "    'format': 'csv'\n",
    "}\n",
    "query = urllib.parse.urlencode(args)\n",
    "url = f\"https://daymet.ornl.gov/single-pixel/api/data?{query}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(url)\n",
    "daymet_df = pd.read_csv(response, header=6)\n",
    "daymet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daymet is annoying in that it only reports the year\n",
    "and day of year, rather than real dates. Also it uses\n",
    "a no-leap calendar, meaning all years have 365 days.\n",
    "So now our goal is to convert the `year` and `yday`\n",
    "columns into proper date times. This one took me a \n",
    "second to figure out, but some quick googling got \n",
    "me there:\n",
    "https://stackoverflow.com/questions/34258892/converting-year-and-day-of-year-into-datetime-index-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestring = (daymet_df['year'].astype(str) \n",
    "              + daymet_df['yday'].astype(str))\n",
    "datestring.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can pass that directly into the `pd.to_datetime` \n",
    "function, with a format of `%Y%j` which means YEAR followed\n",
    "by the Julian day (AKA day of year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(datestring, format='%Y%j')\n",
    "daymet_df.index = pd.DatetimeIndex(dates)\n",
    "daymet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dates correct, we now can merge things together\n",
    "Just one last minor hickup, the lack of leap years in\n",
    "daymet means we have to reindex to the dates from the \n",
    "USGS database. Finally, for shorthand I just rename the whole thing `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verde_df = open_usgs_data(site, begin_date, end_date)\n",
    "daymet_df = daymet_df.reindex(verde_df.index)\n",
    "daymet_df['streamflow'] = verde_df['streamflow']\n",
    "df = daymet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can do some comparisons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = df.resample('M').mean()\n",
    "df_monthly.plot.scatter(x='tmax (deg c)', y='streamflow')\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwrs564a",
   "language": "python",
   "name": "hwrs564a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
